{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_FEATURES_PATH = DATA_RAW / 'train.csv'\n",
    "TRAIN_LABELS_PATH = DATA_RAW / 'train_labels.csv'\n",
    "TEST_PATH = DATA_RAW / 'test.csv'\n",
    "TARGET_PAIRS_PATH = DATA_RAW / 'target_pairs.csv'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data: {DATA_RAW}\")\n",
    "print(f\"Processed data: {DATA_PROCESSED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "features = pd.read_csv(TRAIN_FEATURES_PATH, low_memory=False)\n",
    "labels = pd.read_csv(TRAIN_LABELS_PATH, low_memory=False)\n",
    "test = pd.read_csv(TEST_PATH, low_memory=False)\n",
    "target_pairs = pd.read_csv(TARGET_PAIRS_PATH)\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Target pairs shape: {target_pairs.shape}\")\n",
    "\n",
    "# Check date ranges\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"Features: {features['date_id'].min()} -> {features['date_id'].max()}\")\n",
    "print(f\"Labels: {labels['date_id'].min()} -> {labels['date_id'].max()}\")\n",
    "print(f\"Test: {test['date_id'].min()} -> {test['date_id'].max()}\")\n",
    "\n",
    "# Create proper time series splits with gaps\n",
    "# Training: First 1600 days\n",
    "train_size = 1600\n",
    "\n",
    "# Gap: Next 100 days (to prevent data leakage)\n",
    "gap_size = 100\n",
    "\n",
    "# Validation: Next 90 days\n",
    "val_size = 90\n",
    "\n",
    "# Gap: Next 20 days (to prevent data leakage)\n",
    "gap2_size = 20\n",
    "\n",
    "# Test: Last 90 days (as defined by competition)\n",
    "test_size = 90\n",
    "\n",
    "print(f\"\\nTime series splits with gaps:\")\n",
    "print(f\"Training samples: {train_size} (date_id 0 -> {train_size-1})\")\n",
    "print(f\"Gap 1: {gap_size} days (date_id {train_size} -> {train_size + gap_size - 1})\")\n",
    "print(f\"Validation samples: {val_size} (date_id {train_size + gap_size} -> {train_size + gap_size + val_size - 1})\")\n",
    "print(f\"Gap 2: {gap2_size} days (date_id {train_size + gap_size + val_size} -> {train_size + gap_size + val_size + gap2_size - 1})\")\n",
    "print(f\"Test samples: {test_size} (date_id {test['date_id'].min()} -> {test['date_id'].max()})\")\n",
    "\n",
    "# Split the data\n",
    "train_features = features.iloc[:train_size].copy()\n",
    "train_labels = labels.iloc[:train_size].copy()\n",
    "val_features = features.iloc[train_size + gap_size:train_size + gap_size + val_size].copy()\n",
    "val_labels = labels.iloc[train_size + gap_size:train_size + gap_size + val_size].copy()\n",
    "\n",
    "print(f\"\\nSplit data shapes:\")\n",
    "print(f\"Train features: {train_features.shape}\")\n",
    "print(f\"Train labels: {train_labels.shape}\")\n",
    "print(f\"Validation features: {val_features.shape}\")\n",
    "print(f\"Validation labels: {val_labels.shape}\")\n",
    "print(f\"Test features: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7108d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns\n",
    "feature_cols = [c for c in train_features.columns if c != 'date_id']\n",
    "target_cols = [c for c in train_labels.columns if c != 'date_id']\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Number of targets: {len(target_cols)}\")\n",
    "\n",
    "# Group features by type\n",
    "feature_groups = {\n",
    "    'JPX_Futures': [col for col in feature_cols if 'JPX_' in col],\n",
    "    'LME_Metals': [col for col in feature_cols if 'LME_' in col],\n",
    "    'US_Stocks': [col for col in feature_cols if 'US_Stock_' in col],\n",
    "    'FX_Pairs': [col for col in feature_cols if 'FX_' in col]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature group sizes:\")\n",
    "for group_name, group_cols in feature_groups.items():\n",
    "    print(f\"{group_name}: {len(group_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d351f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log returns for all price features\n",
    "print(\"Creating log returns for training data...\")\n",
    "\n",
    "# Identify price columns (exclude date_id and any non-price columns)\n",
    "price_cols = [col for col in feature_cols if not col.endswith('_missing') and col != 'date_id']\n",
    "\n",
    "print(f\"Creating log returns for {len(price_cols)} price columns\")\n",
    "\n",
    "# Create log returns for TRAINING data only\n",
    "log_returns = np.log(train_features[price_cols] / train_features[price_cols].shift(1))\n",
    "\n",
    "# Rename columns to indicate they're returns\n",
    "log_returns.columns = [f\"{col}_log_return\" for col in price_cols]\n",
    "\n",
    "# Check missing values in log returns\n",
    "missing_log_returns = log_returns.isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values in log returns (top 20):\")\n",
    "print(missing_log_returns.head(20))\n",
    "\n",
    "# Show missing value statistics\n",
    "print(f\"\\nTotal missing values: {log_returns.isna().sum().sum()}\")\n",
    "print(f\"Missing value percentage: {log_returns.isna().sum().sum() / (log_returns.shape[0] * log_returns.shape[1]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical indicator functions\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_bollinger_bands(prices, window=20, num_std=2):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    rolling_mean = prices.rolling(window=window).mean()\n",
    "    rolling_std = prices.rolling(window=window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * num_std)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std)\n",
    "    return upper_band, rolling_mean, lower_band\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD (Moving Average Convergence Divergence)\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a713e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create technical indicators for key instruments\n",
    "print(\"Creating technical indicators for training data...\")\n",
    "\n",
    "technical_features = pd.DataFrame(index=train_features.index)\n",
    "\n",
    "# Key instruments to create indicators for\n",
    "key_instruments = [\n",
    "    'LME_AH_Close',\n",
    "    'JPX_Gold_Standard_Futures_Close',\n",
    "    'US_Stock_VT_adj_close'\n",
    "]\n",
    "\n",
    "for instrument in key_instruments:\n",
    "    if instrument in train_features.columns:\n",
    "        prices = train_features[instrument]  # Use training data only\n",
    "        \n",
    "        # RSI\n",
    "        technical_features[f'{instrument}_rsi_14'] = calculate_rsi(prices, 14)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(prices, 20)\n",
    "        technical_features[f'{instrument}_bb_upper'] = bb_upper\n",
    "        technical_features[f'{instrument}_bb_middle'] = bb_middle\n",
    "        technical_features[f'{instrument}_bb_lower'] = bb_lower\n",
    "        technical_features[f'{instrument}_bb_position'] = (prices - bb_lower) / (bb_upper - bb_lower)\n",
    "        \n",
    "        # MACD\n",
    "        macd_line, signal_line, histogram = calculate_macd(prices)\n",
    "        technical_features[f'{instrument}_macd'] = macd_line\n",
    "        technical_features[f'{instrument}_macd_signal'] = signal_line\n",
    "        technical_features[f'{instrument}_macd_histogram'] = histogram\n",
    "        \n",
    "        print(f\"Created indicators for {instrument}\")\n",
    "    else:\n",
    "        print(f\"Warning: {instrument} not found in features\")\n",
    "\n",
    "print(f\"\\nCreated {len(technical_features.columns)} technical indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features\n",
    "print(\"Creating lagged features for training data...\")\n",
    "\n",
    "lagged_features = pd.DataFrame(index=train_features.index)  # Use training data index\n",
    "\n",
    "# Lag periods to create\n",
    "lag_periods = [1, 5, 10, 20]\n",
    "\n",
    "# Create lags for key instruments\n",
    "for instrument in key_instruments:\n",
    "    if instrument in train_features.columns:\n",
    "        prices = train_features[instrument]  # Use training data only\n",
    "        \n",
    "        for lag in lag_periods:\n",
    "            lagged_features[f'{instrument}_lag_{lag}'] = prices.shift(lag)\n",
    "            \n",
    "        print(f\"Created lags for {instrument}\")\n",
    "    else:\n",
    "        print(f\"Warning: {instrument} not found in features\")\n",
    "\n",
    "print(f\"\\nCreated {len(lagged_features.columns)} lagged features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790633be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling statistics\n",
    "print(\"Creating rolling statistics for training data...\")\n",
    "\n",
    "rolling_features = pd.DataFrame(index=train_features.index)  # Use training data index\n",
    "\n",
    "# Rolling windows\n",
    "rolling_windows = [5, 10, 20]\n",
    "\n",
    "# Create rolling stats for key instruments\n",
    "for instrument in key_instruments:\n",
    "    if instrument in train_features.columns:\n",
    "        prices = train_features[instrument]  # Use training data only\n",
    "        \n",
    "        for window in rolling_windows:\n",
    "            rolling_features[f'{instrument}_rolling_mean_{window}'] = prices.rolling(window).mean()\n",
    "            rolling_features[f'{instrument}_rolling_std_{window}'] = prices.rolling(window).std()\n",
    "            rolling_features[f'{instrument}_rolling_min_{window}'] = prices.rolling(window).min()\n",
    "            rolling_features[f'{instrument}_rolling_max_{window}'] = prices.rolling(window).max()\n",
    "            \n",
    "        print(f\"Created rolling stats for {instrument}\")\n",
    "    else:\n",
    "        print(f\"Warning: {instrument} not found in features\")\n",
    "\n",
    "print(f\"\\nCreated {len(rolling_features.columns)} rolling features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-asset features\n",
    "print(\"Creating cross-asset features for training data...\")\n",
    "\n",
    "cross_asset_features = pd.DataFrame(index=train_features.index)  # Use training data index\n",
    "\n",
    "# Gold-Platinum relationship\n",
    "if 'JPX_Gold_Standard_Futures_Close' in train_features.columns and 'JPX_Platinum_Standard_Futures_Close' in train_features.columns:\n",
    "    gold = train_features['JPX_Gold_Standard_Futures_Close']  # Use training data\n",
    "    platinum = train_features['JPX_Platinum_Standard_Futures_Close']  # Use training data\n",
    "    \n",
    "    cross_asset_features['gold_platinum_spread'] = gold - platinum\n",
    "    cross_asset_features['gold_platinum_ratio'] = gold / platinum\n",
    "    cross_asset_features['gold_platinum_spread_rolling_mean_20'] = cross_asset_features['gold_platinum_spread'].rolling(20).mean()\n",
    "    \n",
    "    print(\"Created Gold-Platinum features\")\n",
    "\n",
    "# Metals index (simple average of LME metals)\n",
    "lme_metals = [col for col in train_features.columns if 'LME_' in col and 'Close' in col]  # Use training data\n",
    "if lme_metals:\n",
    "    metals_data = train_features[lme_metals].fillna(method='ffill')  # Use training data\n",
    "    cross_asset_features['lme_metals_index'] = metals_data.mean(axis=1)\n",
    "    cross_asset_features['lme_metals_volatility'] = metals_data.std(axis=1)\n",
    "    \n",
    "    print(f\"Created LME metals index from {len(lme_metals)} instruments\")\n",
    "\n",
    "print(f\"\\nCreated {len(cross_asset_features.columns)} cross-asset features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580acc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "print(\"Combining all training features...\")\n",
    "\n",
    "# Start with original training features\n",
    "all_features = train_features.copy()  # Use training data\n",
    "\n",
    "# Add log returns\n",
    "all_features = pd.concat([all_features, log_returns], axis=1)\n",
    "print(f\"Added {len(log_returns.columns)} log return features\")\n",
    "\n",
    "# Add technical indicators\n",
    "all_features = pd.concat([all_features, technical_features], axis=1)\n",
    "print(f\"Added {len(technical_features.columns)} technical indicator features\")\n",
    "\n",
    "# Add lagged features\n",
    "all_features = pd.concat([all_features, lagged_features], axis=1)\n",
    "print(f\"Added {len(lagged_features.columns)} lagged features\")\n",
    "\n",
    "# Add rolling features\n",
    "all_features = pd.concat([all_features, rolling_features], axis=1)\n",
    "print(f\"Added {len(rolling_features.columns)} rolling features\")\n",
    "\n",
    "# Add cross-asset features\n",
    "all_features = pd.concat([all_features, cross_asset_features], axis=1)\n",
    "print(f\"Added {len(cross_asset_features.columns)} cross-asset features\")\n",
    "\n",
    "print(f\"\\nFinal training feature matrix shape: {all_features.shape}\")\n",
    "print(f\"Total features: {len(all_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature quality analysis\n",
    "print(\"Analyzing feature quality...\")\n",
    "\n",
    "# Missing value analysis\n",
    "missing_pct = all_features.isna().sum() / len(all_features) * 100\n",
    "print(f\"\\nFeatures with >50% missing values: {(missing_pct > 50).sum()}\")\n",
    "print(f\"Features with >80% missing values: {(missing_pct > 80).sum()}\")\n",
    "\n",
    "# Constant features (no variance)\n",
    "constant_features = []\n",
    "for col in all_features.columns:\n",
    "    if all_features[col].nunique() <= 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "print(f\"\\nConstant features: {len(constant_features)}\")\n",
    "if constant_features:\n",
    "    print(\"Constant features:\", constant_features[:10])\n",
    "\n",
    "# Feature correlation analysis (sample)\n",
    "print(\"\\nAnalyzing feature correlations...\")\n",
    "sample_features = all_features.sample(n=min(100, len(all_features.columns)), axis=1, random_state=42)\n",
    "correlation_matrix = sample_features.corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = np.where(np.abs(correlation_matrix) > 0.95)\n",
    "high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y]) \n",
    "                   for x, y in zip(*high_corr_pairs) if x != y and x < y]\n",
    "\n",
    "print(f\"Highly correlated feature pairs (>0.95): {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    print(\"Sample high correlation pairs:\", high_corr_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to validation data\n",
    "print(\"Applying feature engineering to validation data...\")\n",
    "\n",
    "# Create log returns for validation data\n",
    "val_log_returns = np.log(val_features[price_cols] / val_features[price_cols].shift(1))\n",
    "val_log_returns.columns = [f\"{col}_log_return\" for col in price_cols]\n",
    "\n",
    "# Create technical indicators for validation data\n",
    "val_technical_features = pd.DataFrame(index=val_features.index)\n",
    "for instrument in key_instruments:\n",
    "    if instrument in val_features.columns:\n",
    "        prices = val_features[instrument]\n",
    "        \n",
    "        # RSI\n",
    "        val_technical_features[f'{instrument}_rsi_14'] = calculate_rsi(prices, 14)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(prices, 20)\n",
    "        val_technical_features[f'{instrument}_bb_upper'] = bb_upper\n",
    "        val_technical_features[f'{instrument}_bb_middle'] = bb_middle\n",
    "        val_technical_features[f'{instrument}_bb_lower'] = bb_lower\n",
    "        val_technical_features[f'{instrument}_bb_position'] = (prices - bb_lower) / (bb_upper - bb_lower)\n",
    "        \n",
    "        # MACD\n",
    "        macd_line, signal_line, histogram = calculate_macd(prices)\n",
    "        val_technical_features[f'{instrument}_macd'] = macd_line\n",
    "        val_technical_features[f'{instrument}_macd_signal'] = signal_line\n",
    "        val_technical_features[f'{instrument}_macd_histogram'] = histogram\n",
    "\n",
    "# Create lagged features for validation data\n",
    "val_lagged_features = pd.DataFrame(index=val_features.index)\n",
    "for instrument in key_instruments:\n",
    "    if instrument in val_features.columns:\n",
    "        prices = val_features[instrument]\n",
    "        for lag in lag_periods:\n",
    "            val_lagged_features[f'{instrument}_lag_{lag}'] = prices.shift(lag)\n",
    "\n",
    "# Create rolling features for validation data\n",
    "val_rolling_features = pd.DataFrame(index=val_features.index)\n",
    "for instrument in key_instruments:\n",
    "    if instrument in val_features.columns:\n",
    "        prices = val_features[instrument]\n",
    "        for window in rolling_windows:\n",
    "            val_rolling_features[f'{instrument}_rolling_mean_{window}'] = prices.rolling(window).mean()\n",
    "            val_rolling_features[f'{instrument}_rolling_std_{window}'] = prices.rolling(window).std()\n",
    "            val_rolling_features[f'{instrument}_rolling_min_{window}'] = prices.rolling(window).min()\n",
    "            val_rolling_features[f'{instrument}_rolling_max_{window}'] = prices.rolling(window).max()\n",
    "\n",
    "# Create cross-asset features for validation data\n",
    "val_cross_asset_features = pd.DataFrame(index=val_features.index)\n",
    "if 'JPX_Gold_Standard_Futures_Close' in val_features.columns and 'JPX_Platinum_Standard_Futures_Close' in val_features.columns:\n",
    "    gold = val_features['JPX_Gold_Standard_Futures_Close']\n",
    "    platinum = val_features['JPX_Platinum_Standard_Futures_Close']\n",
    "    \n",
    "    val_cross_asset_features['gold_platinum_spread'] = gold - platinum\n",
    "    val_cross_asset_features['gold_platinum_ratio'] = gold / platinum\n",
    "    val_cross_asset_features['gold_platinum_spread_rolling_mean_20'] = val_cross_asset_features['gold_platinum_spread'].rolling(20).mean()\n",
    "\n",
    "lme_metals = [col for col in val_features.columns if 'LME_' in col and 'Close' in col]\n",
    "if lme_metals:\n",
    "    metals_data = val_features[lme_metals].fillna(method='ffill')\n",
    "    val_cross_asset_features['lme_metals_index'] = metals_data.mean(axis=1)\n",
    "    val_cross_asset_features['lme_metals_volatility'] = metals_data.std(axis=1)\n",
    "\n",
    "# Combine all validation features\n",
    "val_all_features = val_features.copy()\n",
    "val_all_features = pd.concat([val_all_features, val_log_returns], axis=1)\n",
    "val_all_features = pd.concat([val_all_features, val_technical_features], axis=1)\n",
    "val_all_features = pd.concat([val_all_features, val_lagged_features], axis=1)\n",
    "val_all_features = pd.concat([val_all_features, val_rolling_features], axis=1)\n",
    "val_all_features = pd.concat([val_all_features, val_cross_asset_features], axis=1)\n",
    "\n",
    "print(f\"Validation features shape: {val_all_features.shape}\")\n",
    "print(f\"Validation features match training: {val_all_features.shape[1] == all_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f50f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to test data\n",
    "print(\"Applying feature engineering to test data...\")\n",
    "\n",
    "# Create log returns for test data\n",
    "test_log_returns = np.log(test[price_cols] / test[price_cols].shift(1))\n",
    "test_log_returns.columns = [f\"{col}_log_return\" for col in price_cols]\n",
    "\n",
    "# Create technical indicators for test data\n",
    "test_technical_features = pd.DataFrame(index=test.index)\n",
    "for instrument in key_instruments:\n",
    "    if instrument in test.columns:\n",
    "        prices = test[instrument]\n",
    "        \n",
    "        # RSI\n",
    "        test_technical_features[f'{instrument}_rsi_14'] = calculate_rsi(prices, 14)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(prices, 20)\n",
    "        test_technical_features[f'{instrument}_bb_upper'] = bb_upper\n",
    "        test_technical_features[f'{instrument}_bb_middle'] = bb_middle\n",
    "        test_technical_features[f'{instrument}_bb_lower'] = bb_lower\n",
    "        test_technical_features[f'{instrument}_bb_position'] = (prices - bb_lower) / (bb_upper - bb_lower)\n",
    "        \n",
    "        # MACD\n",
    "        macd_line, signal_line, histogram = calculate_macd(prices)\n",
    "        test_technical_features[f'{instrument}_macd'] = macd_line\n",
    "        test_technical_features[f'{instrument}_macd_signal'] = signal_line\n",
    "        test_technical_features[f'{instrument}_macd_histogram'] = histogram\n",
    "\n",
    "# Create lagged features for test data\n",
    "test_lagged_features = pd.DataFrame(index=test.index)\n",
    "for instrument in key_instruments:\n",
    "    if instrument in test.columns:\n",
    "        prices = test[instrument]\n",
    "        for lag in lag_periods:\n",
    "            test_lagged_features[f'{instrument}_lag_{lag}'] = prices.shift(lag)\n",
    "\n",
    "# Create rolling features for test data\n",
    "test_rolling_features = pd.DataFrame(index=test.index)\n",
    "for instrument in key_instruments:\n",
    "    if instrument in test.columns:\n",
    "        prices = test[instrument]\n",
    "        for window in rolling_windows:\n",
    "            test_rolling_features[f'{instrument}_rolling_mean_{window}'] = prices.rolling(window).mean()\n",
    "            test_rolling_features[f'{instrument}_rolling_std_{window}'] = prices.rolling(window).std()\n",
    "            test_rolling_features[f'{instrument}_rolling_min_{window}'] = prices.rolling(window).min()\n",
    "            test_rolling_features[f'{instrument}_rolling_max_{window}'] = prices.rolling(window).max()\n",
    "\n",
    "# Create cross-asset features for test data\n",
    "test_cross_asset_features = pd.DataFrame(index=test.index)\n",
    "if 'JPX_Gold_Standard_Futures_Close' in test.columns and 'JPX_Platinum_Standard_Futures_Close' in test.columns:\n",
    "    gold = test['JPX_Gold_Standard_Futures_Close']\n",
    "    platinum = test['JPX_Platinum_Standard_Futures_Close']\n",
    "    \n",
    "    test_cross_asset_features['gold_platinum_spread'] = gold - platinum\n",
    "    test_cross_asset_features['gold_platinum_ratio'] = gold / platinum\n",
    "    test_cross_asset_features['gold_platinum_spread_rolling_mean_20'] = test_cross_asset_features['gold_platinum_spread'].rolling(20).mean()\n",
    "\n",
    "lme_metals = [col for col in test.columns if 'LME_' in col and 'Close' in col]\n",
    "if lme_metals:\n",
    "    metals_data = test[lme_metals].fillna(method='ffill')\n",
    "    test_cross_asset_features['lme_metals_index'] = metals_data.mean(axis=1)\n",
    "    test_cross_asset_features['lme_metals_volatility'] = metals_data.std(axis=1)\n",
    "\n",
    "# Combine all test features\n",
    "test_all_features = test.copy()\n",
    "test_all_features = pd.concat([test_all_features, test_log_returns], axis=1)\n",
    "test_all_features = pd.concat([test_all_features, test_technical_features], axis=1)\n",
    "test_all_features = pd.concat([test_all_features, test_lagged_features], axis=1)\n",
    "test_all_features = pd.concat([test_all_features, test_rolling_features], axis=1)\n",
    "test_all_features = pd.concat([test_all_features, test_cross_asset_features], axis=1)\n",
    "\n",
    "print(f\"Test features shape: {test_all_features.shape}\")\n",
    "print(f\"Test features match training: {test_all_features.shape[1] == all_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data with proper splits\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save training data (with engineered features)\n",
    "train_features_output_path = DATA_PROCESSED / 'train_features_engineered.csv'\n",
    "train_labels_output_path = DATA_PROCESSED / 'train_labels.csv'\n",
    "val_features_output_path = DATA_PROCESSED / 'val_features_engineered.csv'\n",
    "val_labels_output_path = DATA_PROCESSED / 'val_labels.csv'\n",
    "test_output_path = DATA_PROCESSED / 'test_features_engineered.csv'\n",
    "\n",
    "# Save training data with engineered features\n",
    "all_features.to_csv(train_features_output_path, index=False)\n",
    "train_labels.to_csv(train_labels_output_path, index=False)\n",
    "print(f\"Saved training features (engineered): {train_features_output_path}\")\n",
    "print(f\"Saved training labels: {train_labels_output_path}\")\n",
    "\n",
    "# Save validation data with engineered features\n",
    "val_all_features.to_csv(val_features_output_path, index=False)\n",
    "val_labels.to_csv(val_labels_output_path, index=False)\n",
    "print(f\"Saved validation features (engineered): {val_features_output_path}\")\n",
    "print(f\"Saved validation labels: {val_labels_output_path}\")\n",
    "\n",
    "# Save test data with engineered features\n",
    "test_all_features.to_csv(test_output_path, index=False)\n",
    "print(f\"Saved test features (engineered): {test_output_path}\")\n",
    "\n",
    "print(\"\\nFeature engineering complete!\")\n",
    "print(f\"Final feature count: {len(all_features.columns)}\")\n",
    "print(f\"Training samples: {len(all_features)}\")\n",
    "print(f\"Validation samples: {len(val_all_features)}\")\n",
    "print(f\"Test samples: {len(test_all_features)}\")\n",
    "print(f\"\\nTime series splits:\")\n",
    "print(f\"Training: date_id 0 -> {train_size-1}\")\n",
    "print(f\"Gap 1: date_id {train_size} -> {train_size + gap_size - 1}\")\n",
    "print(f\"Validation: date_id {train_size + gap_size} -> {train_size + gap_size + val_size - 1}\")\n",
    "print(f\"Gap 2: date_id {train_size + gap_size + val_size} -> {train_size + gap_size + val_size + gap2_size - 1}\")\n",
    "print(f\"Test: date_id {test['date_id'].min()} -> {test['date_id'].max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commodity-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
