{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Overview: Mitsui Commodity Prediction\n",
    "\n",
    "# This notebook loads training data, joins on date_id, checks missingness and target coverage,\n",
    "# and verifies target_pairs consistency with basic sanity checks.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('/Users/fw1230/Documents/Projects/mitsui-commodity-prediction')\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "\n",
    "TRAIN_FEATURES_PATH = DATA_RAW / 'train.csv'\n",
    "TRAIN_LABELS_PATH = DATA_RAW / 'train_labels.csv'\n",
    "TEST_PATH = DATA_RAW / 'test.csv'\n",
    "TARGET_PAIRS_PATH = DATA_RAW / 'target_pairs.csv'\n",
    "\n",
    "print(TRAIN_FEATURES_PATH)\n",
    "print(TRAIN_LABELS_PATH)\n",
    "print(TEST_PATH)\n",
    "print(TARGET_PAIRS_PATH)\n",
    "\n",
    "assert TRAIN_FEATURES_PATH.exists(), 'train.csv missing'\n",
    "assert TRAIN_LABELS_PATH.exists(), 'train_labels.csv missing'\n",
    "assert TEST_PATH.exists(), 'test.csv missing'\n",
    "assert TARGET_PAIRS_PATH.exists(), 'target_pairs.csv missing'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "features = pd.read_csv(TRAIN_FEATURES_PATH, low_memory=False)\n",
    "labels = pd.read_csv(TRAIN_LABELS_PATH, low_memory=False)\n",
    "test = pd.read_csv(TEST_PATH, low_memory=False)\n",
    "target_pairs = pd.read_csv(TARGET_PAIRS_PATH)\n",
    "\n",
    "print('features shape:', features.shape)\n",
    "print('labels shape:', labels.shape)\n",
    "print('test shape:', test.shape)\n",
    "print('target_pairs shape:', target_pairs.shape)\n",
    "\n",
    "print('features date_id range:', int(features['date_id'].min()), '->', int(features['date_id'].max()))\n",
    "print('labels date_id range:', int(labels['date_id'].min()), '->', int(labels['date_id'].max()))\n",
    "print('test date_id range   :', int(test['date_id'].min()), '->', int(test['date_id'].max()))\n",
    "\n",
    "features.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join features and labels on date_id\n",
    "merged = features.merge(labels, on='date_id', how='left', validate='one_to_one')\n",
    "print('merged shape:', merged.shape)\n",
    "\n",
    "# Identify feature and target columns\n",
    "feature_cols = [c for c in features.columns if c != 'date_id']\n",
    "target_cols = [c for c in labels.columns if c != 'date_id']\n",
    "\n",
    "print('num features:', len(feature_cols))\n",
    "print('num targets :', len(target_cols))\n",
    "\n",
    "merged.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness summary\n",
    "missing_feat_pct = features[feature_cols].isna().mean().sort_values(ascending=False)\n",
    "missing_tgt_pct = labels[target_cols].isna().mean().sort_values(ascending=False)\n",
    "\n",
    "print('Top 20 missing features:')\n",
    "display(missing_feat_pct.head(20))\n",
    "print('\\nTop 20 missing targets:')\n",
    "display(missing_tgt_pct.head(20))\n",
    "\n",
    "# Coverage of targets (non-null counts)\n",
    "coverage = labels[target_cols].notna().sum().sort_values(ascending=False)\n",
    "print('\\nTarget coverage (non-null counts) - top 20:')\n",
    "display(coverage.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected target_pairs consistency check - forward-looking log returns\n",
    "pairs = target_pairs.copy()\n",
    "pairs = pairs[pairs['target'].str.startswith('target_')].reset_index(drop=True)\n",
    "\n",
    "# Parse pair string\n",
    "pairs['symbol_a'] = pairs['pair'].str.split('-').str[0].str.strip()\n",
    "pairs['symbol_b'] = pairs['pair'].where(~pairs['pair'].str.contains('-'), \n",
    "                                       pairs['pair'].str.split('-').str[1]).str.strip()\n",
    "\n",
    "# Sample for faster computation\n",
    "sample_pairs = pairs.sample(min(15, len(pairs)), random_state=42)\n",
    "\n",
    "results = []\n",
    "for _, row in sample_pairs.iterrows():\n",
    "    tcol = row['target']\n",
    "    lag = int(row['lag']) if not pd.isna(row['lag']) else 1\n",
    "    a = row['symbol_a']\n",
    "    b = row['symbol_b'] if isinstance(row['symbol_b'], str) else None\n",
    "    \n",
    "    if a not in features.columns:\n",
    "        continue\n",
    "        \n",
    "    # Compute the target according to the actual generation code\n",
    "    tmp = features[['date_id', a]].copy()\n",
    "    \n",
    "    if b and b in features.columns:\n",
    "        # Case 2: Difference of log returns for pairs\n",
    "        # log(price_A_{t+lag+1} / price_A_{t+1}) - log(price_B_{t+lag+1} / price_B_{t+1})\n",
    "        tmp['a_return'] = np.log(features[a].shift(-(lag+1)) / features[a].shift(-1))\n",
    "        tmp['b_return'] = np.log(features[b].shift(-(lag+1)) / features[b].shift(-1))\n",
    "        tmp['computed_target'] = tmp['a_return'] - tmp['b_return']\n",
    "    else:\n",
    "        # Case 1: Log return for single instrument\n",
    "        # log(price_{t+lag+1} / price_{t+1})\n",
    "        tmp['computed_target'] = np.log(features[a].shift(-(lag+1)) / features[a].shift(-1))\n",
    "    \n",
    "    # Merge with actual target\n",
    "    tmp = tmp.merge(labels[['date_id', tcol]], on='date_id', how='inner')\n",
    "    tmp = tmp.sort_values('date_id')\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr = tmp[['computed_target', tcol]].dropna().corr().iloc[0,1]\n",
    "    n_samples = tmp[['computed_target', tcol]].dropna().shape[0]\n",
    "    \n",
    "    results.append({\n",
    "        'target': tcol, \n",
    "        'lag': lag, \n",
    "        'symbol_a': a, \n",
    "        'symbol_b': b, \n",
    "        'type': 'pair_returns' if b else 'single_return',\n",
    "        'n': n_samples, \n",
    "        'corr': corr\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results).sort_values('corr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distributions and correlations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of a few sample targets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "sample_targets = ['target_0', 'target_1', 'target_2', 'target_3']\n",
    "\n",
    "for i, target in enumerate(sample_targets):\n",
    "    if target in merged.columns:\n",
    "        row, col = i // 2, i % 2\n",
    "        merged[target].dropna().hist(bins=50, ax=axes[row, col], alpha=0.7)\n",
    "        axes[row, col].set_title(f'{target} Distribution')\n",
    "        axes[row, col].set_xlabel('Value')\n",
    "        axes[row, col].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Target correlation heatmap (first 20 targets)\n",
    "target_subset = [col for col in target_cols[:20] if col in merged.columns]\n",
    "corr_matrix = merged[target_subset].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='RdBu_r', center=0)\n",
    "plt.title('Target Correlations (First 20)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group features by type and get summary stats\n",
    "feature_groups = {\n",
    "    'JPX_Futures': [col for col in feature_cols if 'JPX_' in col],\n",
    "    'LME_Metals': [col for col in feature_cols if 'LME_' in col],\n",
    "    'US_Stocks': [col for col in feature_cols if 'US_Stock_' in col],\n",
    "    'FX_Pairs': [col for col in feature_cols if 'FX_' in col]\n",
    "}\n",
    "\n",
    "print(\"Feature Group Sizes:\")\n",
    "for group_name, group_cols in feature_groups.items():\n",
    "    print(f\"{group_name}: {len(group_cols)} features\")\n",
    "\n",
    "# Summary stats for each group\n",
    "for group_name, group_cols in feature_groups.items():\n",
    "    if group_cols:\n",
    "        print(f\"\\n{group_name} Summary Stats:\")\n",
    "        print(merged[group_cols].describe())\n",
    "\n",
    "for group_name, group_cols in feature_groups.items():\n",
    "    if len(group_cols) > 1:\n",
    "        print(f\"\\n{group_name} Correlation Matrix (first 10 features):\")\n",
    "        subset_cols = group_cols[:10]  # Limit to avoid huge matrices\n",
    "        corr_matrix = merged[subset_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, fmt='.2f')\n",
    "        plt.title(f'{group_name} Feature Correlations')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key features over time\n",
    "key_features = ['LME_AH_Close', 'JPX_Gold_Standard_Futures_Close', 'US_Stock_VT_adj_close']\n",
    "\n",
    "fig, axes = plt.subplots(len(key_features), 1, figsize=(12, 10))\n",
    "for i, feature in enumerate(key_features):\n",
    "    if feature in merged.columns:\n",
    "        merged.plot(x='date_id', y=feature, ax=axes[i], legend=False)\n",
    "        axes[i].set_title(f'{feature} Over Time')\n",
    "        axes[i].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Volatility analysis (rolling std) - CORRECTED\n",
    "fig, axes = plt.subplots(len(key_features), 1, figsize=(12, 10))\n",
    "for i, feature in enumerate(key_features):\n",
    "    if feature in merged.columns:\n",
    "        rolling_std = merged[feature].rolling(5).std()\n",
    "        axes[i].plot(merged['date_id'], rolling_std)\n",
    "        axes[i].set_title(f'{feature} Rolling 20-period Std Dev')\n",
    "        axes[i].set_ylabel('Volatility')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value patterns by group\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for i, (group_name, group_cols) in enumerate(feature_groups.items()):\n",
    "    if group_cols:\n",
    "        row, col = i // 2, i % 2\n",
    "        missing_pct = merged[group_cols].isna().mean().sort_values(ascending=False)\n",
    "        missing_pct.head(10).plot(kind='bar', ax=axes[row, col])\n",
    "        axes[row, col].set_title(f'{group_name} Missing Values (%)')\n",
    "        axes[row, col].set_ylabel('Missing %')\n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify price columns (exclude date_id and any non-price columns)\n",
    "price_cols = [col for col in feature_cols if not col.endswith('_missing') and col != 'date_id']\n",
    "\n",
    "print(f\"Creating log returns for {len(price_cols)} price columns\")\n",
    "\n",
    "# Create log returns\n",
    "log_returns = np.log(features[price_cols] / features[price_cols].shift(1))\n",
    "\n",
    "# Rename columns to indicate they're returns\n",
    "log_returns.columns = [f\"{col}_log_return\" for col in price_cols]\n",
    "\n",
    "# Check missing values in log returns\n",
    "missing_log_returns = log_returns.isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values in log returns (top 20):\")\n",
    "print(missing_log_returns.head(20))\n",
    "\n",
    "# Forward fill missing values (but leave high-missing features as-is)\n",
    "log_returns_filled = log_returns.copy()\n",
    "\n",
    "# Only forward fill features with reasonable missing rates (<50%)\n",
    "reasonable_missing = missing_log_returns[missing_log_returns <= len(log_returns) * 0.5]\n",
    "high_missing = missing_log_returns[missing_log_returns > len(log_returns) * 0.5]\n",
    "\n",
    "print(f\"\\nFeatures with reasonable missing rates (<50%): {len(reasonable_missing)}\")\n",
    "print(f\"Features with high missing rates (>50%): {len(high_missing)}\")\n",
    "\n",
    "# Forward fill only the reasonable ones\n",
    "log_returns_filled[reasonable_missing.index] = log_returns_filled[reasonable_missing.index].fillna(method='ffill')\n",
    "\n",
    "# Combine original features with log returns\n",
    "features_with_returns = pd.concat([\n",
    "    features.reset_index(drop=True),\n",
    "    log_returns_filled.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nFinal features shape: {features_with_returns.shape}\")\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"New log return features: {len(log_returns.columns)}\")\n",
    "\n",
    "# Show some examples of the new features\n",
    "print(\"\\nSample of new log return features:\")\n",
    "print(features_with_returns[['date_id'] + list(log_returns.columns[:5])].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commodity-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
