{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Import competition metrics\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.metrics import calculate_competition_score, interpret_competition_score\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models' / 'baseline'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "PREDICTIONS_DIR = PROJECT_ROOT / 'models' / 'predictions'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREDICTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Processed data: {DATA_PROCESSED}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Predictions directory: {PREDICTIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe07b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from notebook 02\n",
    "print(\"Loading processed data...\")\n",
    "\n",
    "# Check if processed data exists with new structure\n",
    "train_features_path = DATA_PROCESSED / 'train_features_engineered.csv'\n",
    "train_labels_path = DATA_PROCESSED / 'train_labels.csv'\n",
    "val_features_path = DATA_PROCESSED / 'val_features_engineered.csv'\n",
    "val_labels_path = DATA_PROCESSED / 'val_labels.csv'\n",
    "test_features_path = DATA_PROCESSED / 'test_features_engineered.csv'\n",
    "\n",
    "if train_features_path.exists() and val_features_path.exists():\n",
    "    # Load processed data with train/val split\n",
    "    train_features_engineered = pd.read_csv(train_features_path)\n",
    "    train_labels = pd.read_csv(train_labels_path)\n",
    "    val_features_engineered = pd.read_csv(val_features_path)\n",
    "    val_labels = pd.read_csv(val_labels_path)\n",
    "    test_features_engineered = pd.read_csv(test_features_path)\n",
    "    \n",
    "    print(\"Loaded processed data with train/validation split\")\n",
    "    print(f\"Training features: {train_features_engineered.shape}\")\n",
    "    print(f\"Training labels: {train_labels.shape}\")\n",
    "    print(f\"Validation features: {val_features_engineered.shape}\")\n",
    "    print(f\"Validation labels: {val_labels.shape}\")\n",
    "    print(f\"Test features: {test_features_engineered.shape}\")\n",
    "    \n",
    "    # Extract log returns for training data (baseline uses only log returns)\n",
    "    log_return_cols = [col for col in train_features_engineered.columns if 'log_return' in col]\n",
    "    print(f\"Found {len(log_return_cols)} log return columns\")\n",
    "    \n",
    "    if len(log_return_cols) > 0:\n",
    "        train_log_returns = train_features_engineered[['date_id'] + log_return_cols]\n",
    "        val_log_returns = val_features_engineered[['date_id'] + log_return_cols]\n",
    "        print(f\"Successfully extracted log returns\")\n",
    "    else:\n",
    "        print(\"No log return columns found in processed data!\")\n",
    "        raise ValueError(\"No log returns found in processed data\")\n",
    "    \n",
    "else:\n",
    "    print(\"Processed data not found. Please run notebook 02 first.\")\n",
    "    raise FileNotFoundError(\"Processed data not found\")\n",
    "\n",
    "print(f\"\\nData summary:\")\n",
    "print(f\"Training log returns: {train_log_returns.shape}\")\n",
    "print(f\"Validation log returns: {val_log_returns.shape}\")\n",
    "print(f\"Training labels: {train_labels.shape}\")\n",
    "print(f\"Validation labels: {val_labels.shape}\")\n",
    "\n",
    "# Verify we have log returns\n",
    "print(f\"\\nLog return columns: {len([col for col in train_log_returns.columns if 'log_return' in col])}\")\n",
    "print(f\"Sample log return columns: {[col for col in train_log_returns.columns if 'log_return' in col][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaafbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"Preparing data for modeling...\")\n",
    "\n",
    "# Get target columns\n",
    "target_cols = [c for c in train_labels.columns if c != 'date_id']\n",
    "print(f\"Number of targets: {len(target_cols)}\")\n",
    "\n",
    "# Prepare feature matrix for training (log returns only)\n",
    "feature_cols = [col for col in train_log_returns.columns if col != 'date_id']\n",
    "X_train = train_log_returns[feature_cols]\n",
    "y_train = train_labels[target_cols]\n",
    "\n",
    "# Prepare feature matrix for validation (log returns only)\n",
    "X_val = val_log_returns[feature_cols]\n",
    "y_val = val_labels[target_cols]\n",
    "\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Training target matrix shape: {y_train.shape}\")\n",
    "print(f\"Validation feature matrix shape: {X_val.shape}\")\n",
    "print(f\"Validation target matrix shape: {y_val.shape}\")\n",
    "print(f\"Feature-to-sample ratio (training): {X_train.shape[1] / X_train.shape[0]:.3f}\")\n",
    "\n",
    "# Check missing values\n",
    "train_missing_pct = X_train.isna().sum().sum() / (X_train.shape[0] * X_train.shape[1]) * 100\n",
    "val_missing_pct = X_val.isna().sum().sum() / (X_val.shape[0] * X_val.shape[1]) * 100\n",
    "print(f\"Missing values in training features: {train_missing_pct:.2f}%\")\n",
    "print(f\"Missing values in validation features: {val_missing_pct:.2f}%\")\n",
    "\n",
    "# Check target missing values\n",
    "train_target_missing_pct = y_train.isna().sum().sum() / (y_train.shape[0] * y_train.shape[1]) * 100\n",
    "val_target_missing_pct = y_val.isna().sum().sum() / (y_val.shape[0] * y_val.shape[1]) * 100\n",
    "print(f\"Missing values in training targets: {train_target_missing_pct:.2f}%\")\n",
    "print(f\"Missing values in validation targets: {val_target_missing_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206de50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model training (start with first 10 targets)\n",
    "print(\"Training baseline models...\")\n",
    "\n",
    "# Start with first 10 targets for quick testing\n",
    "test_targets = target_cols[:10]\n",
    "print(f\"Training models for targets: {test_targets}\")\n",
    "\n",
    "# Store results\n",
    "models = {}\n",
    "scores = {}\n",
    "training_times = {}\n",
    "feature_importance = {}\n",
    "\n",
    "# Note: Using single validation set approach (no cross-validation on training data)\n",
    "# This is more appropriate for time series data with proper train/validation splits\n",
    "\n",
    "for i, target in enumerate(test_targets):\n",
    "    print(f\"\\nTraining model {i+1}/{len(test_targets)} for {target}...\")\n",
    "    \n",
    "    # Get target data (remove missing values)\n",
    "    train_target_data = y_train[target].dropna()\n",
    "    train_feature_data = X_train.loc[train_target_data.index]\n",
    "    \n",
    "    print(f\"  Training samples: {len(train_target_data)}\")\n",
    "    print(f\"  Features: {train_feature_data.shape[1]}\")\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # Train model on all training data (no cross-validation)\n",
    "    model.fit(train_feature_data, train_target_data)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    models[target] = model\n",
    "    scores[target] = 0.0  # We'll calculate this on validation set\n",
    "    training_times[target] = training_time\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.feature_importances_\n",
    "    feature_importance[target] = pd.DataFrame({\n",
    "        'feature': train_feature_data.columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"  Top feature: {feature_importance[target].iloc[0]['feature']} ({feature_importance[target].iloc[0]['importance']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5249e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"Baseline Model Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "performance_df = pd.DataFrame({\n",
    "    'target': list(models.keys()),\n",
    "    'training_time': list(training_times.values())\n",
    "})\n",
    "\n",
    "print(performance_df)\n",
    "print(f\"\\nAverage training time: {performance_df['training_time'].mean():.2f}s\")\n",
    "print(f\"Total training time: {performance_df['training_time'].sum():.2f}s\")\n",
    "print(f\"Models trained: {len(models)}\")\n",
    "\n",
    "# Note: Performance metrics will be calculated on validation set\n",
    "print(\"\\nNote: Model performance will be evaluated on validation set using competition metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine all feature importance\n",
    "all_importance = pd.concat(feature_importance.values(), keys=feature_importance.keys())\n",
    "all_importance = all_importance.reset_index()\n",
    "\n",
    "# Check the actual column names\n",
    "print(\"Actual column names:\", all_importance.columns.tolist())\n",
    "\n",
    "# Rename columns based on what we actually have\n",
    "if len(all_importance.columns) == 4:\n",
    "    # If we have 4 columns, it's likely: level_0, level_1, feature, importance\n",
    "    all_importance.columns = ['target', 'index', 'feature', 'importance']\n",
    "    # Drop the index column if it's not needed\n",
    "    all_importance = all_importance.drop('index', axis=1)\n",
    "elif len(all_importance.columns) == 3:\n",
    "    # If we have 3 columns, it's likely: level_0, feature, importance\n",
    "    all_importance.columns = ['target', 'feature', 'importance']\n",
    "else:\n",
    "    # Let's see what we actually have\n",
    "    print(\"Unexpected number of columns. Actual columns:\")\n",
    "    print(all_importance.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(all_importance.head())\n",
    "\n",
    "# Overall feature importance (average across targets)\n",
    "overall_importance = all_importance.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 most important features overall:\")\n",
    "print(overall_importance.head(20))\n",
    "\n",
    "# Feature importance by target\n",
    "print(\"\\nTop 5 features for each target:\")\n",
    "for target in test_targets:\n",
    "    print(f\"\\n{target}:\")\n",
    "    top_features = feature_importance[target].head(5)\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "print(\"Saving models and results...\")\n",
    "\n",
    "# Save trained models\n",
    "for target in models.keys():\n",
    "    model_path = MODELS_DIR / f'{target}_model.joblib'\n",
    "    joblib.dump(models[target], model_path)\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "\n",
    "# Save training summary (no performance metrics since we're not using CV)\n",
    "training_summary = pd.DataFrame({\n",
    "    'target': list(models.keys()),\n",
    "    'training_time': list(training_times.values())\n",
    "})\n",
    "training_summary_path = RESULTS_DIR / 'baseline_training_summary.csv'\n",
    "training_summary.to_csv(training_summary_path, index=False)\n",
    "print(f\"Saved training summary: {training_summary_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = RESULTS_DIR / 'feature_importance_baseline.csv'\n",
    "all_importance.to_csv(importance_path, index=False)\n",
    "print(f\"Saved feature importance: {importance_path}\")\n",
    "\n",
    "# Save overall importance summary\n",
    "overall_importance_path = RESULTS_DIR / 'feature_importance_summary.csv'\n",
    "overall_importance.to_frame().to_csv(overall_importance_path)\n",
    "print(f\"Saved overall importance: {overall_importance_path}\")\n",
    "\n",
    "print(f\"\\nModels and results saved successfully!\")\n",
    "print(f\"Note: Performance evaluation will be done on validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on validation set with competition metric\n",
    "print(\"Generating predictions on validation set...\")\n",
    "\n",
    "# Use the already loaded validation data with log returns only\n",
    "val_feature_cols = [col for col in val_log_returns.columns if col != 'date_id']\n",
    "X_val = val_log_returns[val_feature_cols]\n",
    "y_val = val_labels[target_cols]\n",
    "\n",
    "print(f\"Validation log returns shape: {val_log_returns.shape}\")\n",
    "print(f\"Validation feature matrix shape: {X_val.shape}\")\n",
    "print(f\"Validation target matrix shape: {y_val.shape}\")\n",
    "\n",
    "# Generate predictions on validation set\n",
    "val_predictions = {}\n",
    "val_metrics = {}\n",
    "\n",
    "for target in models.keys():\n",
    "    print(f\"Generating predictions for {target}...\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    val_features_clean = X_val.fillna(0)\n",
    "    \n",
    "    # Ensure we have the same features as training\n",
    "    missing_features = set(feature_cols) - set(val_features_clean.columns)\n",
    "    if missing_features:\n",
    "        print(f\"  Warning: Missing {len(missing_features)} features, adding zeros\")\n",
    "        for feature in missing_features:\n",
    "            val_features_clean[feature] = 0\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    val_features_clean = val_features_clean[feature_cols]\n",
    "    \n",
    "    # Make predictions\n",
    "    val_predictions[target] = models[target].predict(val_features_clean)\n",
    "    \n",
    "    # Calculate traditional metrics\n",
    "    true_vals = y_val[target].dropna()\n",
    "    pred_vals = val_predictions[target][:len(true_vals)]\n",
    "    \n",
    "    if len(true_vals) > 0:\n",
    "        rmse = np.sqrt(mean_squared_error(true_vals, pred_vals))\n",
    "        mae = mean_absolute_error(true_vals, pred_vals)\n",
    "        r2 = r2_score(true_vals, pred_vals)\n",
    "        \n",
    "        val_metrics[target] = {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    else:\n",
    "        print(f\"  No valid target values for {target}\")\n",
    "\n",
    "# Create predictions DataFrame for competition metric\n",
    "val_predictions_df = pd.DataFrame(val_predictions)\n",
    "val_predictions_df['date_id'] = val_log_returns['date_id']\n",
    "val_predictions_df = val_predictions_df[['date_id'] + list(val_predictions.keys())]\n",
    "\n",
    "# Calculate competition score\n",
    "print(f\"\\nCalculating competition score...\")\n",
    "competition_score = calculate_competition_score(y_val, val_predictions_df[list(models.keys())])\n",
    "print(f\"Competition Score: {competition_score:.4f}\")\n",
    "print(f\"Score Interpretation: {interpret_competition_score(competition_score)}\")\n",
    "\n",
    "# Save validation predictions\n",
    "val_predictions_path = RESULTS_DIR / 'baseline' / 'predictions.csv'\n",
    "val_predictions_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "val_predictions_df.to_csv(val_predictions_path, index=False)\n",
    "print(f\"Saved validation predictions: {val_predictions_path}\")\n",
    "\n",
    "# Save validation metrics\n",
    "val_metrics_df = pd.DataFrame(val_metrics).T\n",
    "val_metrics_df.index.name = 'target'\n",
    "val_metrics_df.reset_index(inplace=True)\n",
    "val_metrics_path = RESULTS_DIR / 'baseline' / 'validation_metrics.csv'\n",
    "val_metrics_df.to_csv(val_metrics_path, index=False)\n",
    "print(f\"Saved validation metrics: {val_metrics_path}\")\n",
    "\n",
    "# Save competition score\n",
    "competition_score_path = RESULTS_DIR / 'baseline' / 'competition_score.txt'\n",
    "with open(competition_score_path, 'w') as f:\n",
    "    f.write(f\"Competition Score: {competition_score:.4f}\\n\")\n",
    "    f.write(f\"Interpretation: {interpret_competition_score(competition_score)}\")\n",
    "print(f\"Saved competition score: {competition_score_path}\")\n",
    "\n",
    "print(f\"\\nValidation predictions shape: {val_predictions_df.shape}\")\n",
    "print(f\"Validation metrics calculated for {len(val_metrics)} targets\")\n",
    "print(f\"Competition Score: {competition_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"Baseline Model Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Models trained: {len(models)}\")\n",
    "print(f\"Average training time: {performance_df['training_time'].mean():.2f}s\")\n",
    "print(f\"Total training time: {performance_df['training_time'].sum():.2f}s\")\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "for i, (feature, importance) in enumerate(overall_importance.head(10).items()):\n",
    "    print(f\"{i+1:2d}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Load and display competition score if available\n",
    "competition_score_path = RESULTS_DIR / 'baseline' / 'competition_score.txt'\n",
    "if competition_score_path.exists():\n",
    "    with open(competition_score_path, 'r') as f:\n",
    "        score_content = f.read()\n",
    "        print(f\"\\nCompetition Score Results:\")\n",
    "        print(score_content)\n",
    "else:\n",
    "    print(f\"\\nNote: Run validation prediction cell to get competition score\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Analyze competition score and feature importance patterns\")\n",
    "print(\"2. Train models for all 425 targets\")\n",
    "print(\"3. Try using all engineered features (technical indicators, lagged features, etc.)\")\n",
    "print(\"4. Experiment with different algorithms (Random Forest, Neural Networks)\")\n",
    "print(\"5. Try factor models (PCA) for dimensionality reduction\")\n",
    "print(\"6. Ensemble different approaches\")\n",
    "print(\"7. Feature selection to reduce noise\")\n",
    "print(\"8. Hyperparameter tuning for better rank correlation\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "print(f\"Models saved to: {MODELS_DIR}\")\n",
    "print(f\"Predictions saved to: {PREDICTIONS_DIR}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"• Using log returns only as baseline (557 features)\")\n",
    "print(f\"• Competition metric: Rank Correlation Sharpe Ratio\")\n",
    "print(f\"• Time series split with gaps to prevent data leakage\")\n",
    "print(f\"• Single validation approach (no cross-validation)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commodity-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
