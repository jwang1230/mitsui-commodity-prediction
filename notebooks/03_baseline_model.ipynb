{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c43c9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Import competition metrics\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.metrics import calculate_competition_score, interpret_competition_score\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc51f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction\n",
      "Processed data: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/data/processed\n",
      "Models directory: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline\n",
      "Results directory: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results\n",
      "Predictions directory: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/predictions\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models' / 'baseline'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "PREDICTIONS_DIR = PROJECT_ROOT / 'models' / 'predictions'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREDICTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Processed data: {DATA_PROCESSED}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Predictions directory: {PREDICTIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe07b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Loaded processed data with train/validation split\n",
      "Training features: (1600, 1192)\n",
      "Training labels: (1600, 425)\n",
      "Validation features: (90, 1192)\n",
      "Validation labels: (90, 425)\n",
      "Test features: (90, 1193)\n",
      "Found 557 log return columns\n",
      "Successfully extracted log returns\n",
      "\n",
      "Data summary:\n",
      "Training log returns: (1600, 558)\n",
      "Validation log returns: (90, 558)\n",
      "Training labels: (1600, 425)\n",
      "Validation labels: (90, 425)\n",
      "\n",
      "Log return columns: 557\n",
      "Sample log return columns: ['LME_AH_Close_log_return', 'LME_CA_Close_log_return', 'LME_PB_Close_log_return', 'LME_ZS_Close_log_return', 'JPX_Gold_Mini_Futures_Open_log_return']\n"
     ]
    }
   ],
   "source": [
    "# Load processed data from notebook 02\n",
    "print(\"Loading processed data...\")\n",
    "\n",
    "# Check if processed data exists with new structure\n",
    "train_features_path = DATA_PROCESSED / 'train_features_engineered.csv'\n",
    "train_labels_path = DATA_PROCESSED / 'train_labels.csv'\n",
    "val_features_path = DATA_PROCESSED / 'val_features_engineered.csv'\n",
    "val_labels_path = DATA_PROCESSED / 'val_labels.csv'\n",
    "test_features_path = DATA_PROCESSED / 'test_features_engineered.csv'\n",
    "\n",
    "if train_features_path.exists() and val_features_path.exists():\n",
    "    # Load processed data with train/val split\n",
    "    train_features_engineered = pd.read_csv(train_features_path)\n",
    "    train_labels = pd.read_csv(train_labels_path)\n",
    "    val_features_engineered = pd.read_csv(val_features_path)\n",
    "    val_labels = pd.read_csv(val_labels_path)\n",
    "    test_features_engineered = pd.read_csv(test_features_path)\n",
    "    \n",
    "    print(\"Loaded processed data with train/validation split\")\n",
    "    print(f\"Training features: {train_features_engineered.shape}\")\n",
    "    print(f\"Training labels: {train_labels.shape}\")\n",
    "    print(f\"Validation features: {val_features_engineered.shape}\")\n",
    "    print(f\"Validation labels: {val_labels.shape}\")\n",
    "    print(f\"Test features: {test_features_engineered.shape}\")\n",
    "    \n",
    "    # Extract log returns for training data (baseline uses only log returns)\n",
    "    log_return_cols = [col for col in train_features_engineered.columns if 'log_return' in col]\n",
    "    print(f\"Found {len(log_return_cols)} log return columns\")\n",
    "    \n",
    "    if len(log_return_cols) > 0:\n",
    "        train_log_returns = train_features_engineered[['date_id'] + log_return_cols]\n",
    "        val_log_returns = val_features_engineered[['date_id'] + log_return_cols]\n",
    "        print(f\"Successfully extracted log returns\")\n",
    "    else:\n",
    "        print(\"No log return columns found in processed data!\")\n",
    "        raise ValueError(\"No log returns found in processed data\")\n",
    "    \n",
    "else:\n",
    "    print(\"Processed data not found. Please run notebook 02 first.\")\n",
    "    raise FileNotFoundError(\"Processed data not found\")\n",
    "\n",
    "print(f\"\\nData summary:\")\n",
    "print(f\"Training log returns: {train_log_returns.shape}\")\n",
    "print(f\"Validation log returns: {val_log_returns.shape}\")\n",
    "print(f\"Training labels: {train_labels.shape}\")\n",
    "print(f\"Validation labels: {val_labels.shape}\")\n",
    "\n",
    "# Verify we have log returns\n",
    "print(f\"\\nLog return columns: {len([col for col in train_log_returns.columns if 'log_return' in col])}\")\n",
    "print(f\"Sample log return columns: {[col for col in train_log_returns.columns if 'log_return' in col][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afaafbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for modeling...\n",
      "Number of targets: 424\n",
      "Training feature matrix shape: (1600, 557)\n",
      "Training target matrix shape: (1600, 424)\n",
      "Validation feature matrix shape: (90, 557)\n",
      "Validation target matrix shape: (90, 424)\n",
      "Feature-to-sample ratio (training): 0.348\n",
      "Missing values in training features: 7.22%\n",
      "Missing values in validation features: 4.69%\n",
      "Missing values in training targets: 10.51%\n",
      "Missing values in validation targets: 7.25%\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"Preparing data for modeling...\")\n",
    "\n",
    "# Get target columns\n",
    "target_cols = [c for c in train_labels.columns if c != 'date_id']\n",
    "print(f\"Number of targets: {len(target_cols)}\")\n",
    "\n",
    "# Prepare feature matrix for training (log returns only)\n",
    "feature_cols = [col for col in train_log_returns.columns if col != 'date_id']\n",
    "X_train = train_log_returns[feature_cols]\n",
    "y_train = train_labels[target_cols]\n",
    "\n",
    "# Prepare feature matrix for validation (log returns only)\n",
    "X_val = val_log_returns[feature_cols]\n",
    "y_val = val_labels[target_cols]\n",
    "\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Training target matrix shape: {y_train.shape}\")\n",
    "print(f\"Validation feature matrix shape: {X_val.shape}\")\n",
    "print(f\"Validation target matrix shape: {y_val.shape}\")\n",
    "print(f\"Feature-to-sample ratio (training): {X_train.shape[1] / X_train.shape[0]:.3f}\")\n",
    "\n",
    "# Check missing values\n",
    "train_missing_pct = X_train.isna().sum().sum() / (X_train.shape[0] * X_train.shape[1]) * 100\n",
    "val_missing_pct = X_val.isna().sum().sum() / (X_val.shape[0] * X_val.shape[1]) * 100\n",
    "print(f\"Missing values in training features: {train_missing_pct:.2f}%\")\n",
    "print(f\"Missing values in validation features: {val_missing_pct:.2f}%\")\n",
    "\n",
    "# Check target missing values\n",
    "train_target_missing_pct = y_train.isna().sum().sum() / (y_train.shape[0] * y_train.shape[1]) * 100\n",
    "val_target_missing_pct = y_val.isna().sum().sum() / (y_val.shape[0] * y_val.shape[1]) * 100\n",
    "print(f\"Missing values in training targets: {train_target_missing_pct:.2f}%\")\n",
    "print(f\"Missing values in validation targets: {val_target_missing_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "206de50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline models...\n",
      "Training models for targets: ['target_0', 'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6', 'target_7', 'target_8', 'target_9']\n",
      "\n",
      "Training model 1/10 for target_0...\n",
      "  Training samples: 1494\n",
      "  Features: 557\n",
      "  Training time: 4.77s\n",
      "  Top feature: US_Stock_MPC_adj_low_log_return (0.0300)\n",
      "\n",
      "Training model 2/10 for target_1...\n",
      "  Training samples: 1456\n",
      "  Features: 557\n",
      "  Training time: 4.56s\n",
      "  Top feature: US_Stock_MPC_adj_low_log_return (0.0254)\n",
      "\n",
      "Training model 3/10 for target_2...\n",
      "  Training samples: 1529\n",
      "  Features: 557\n",
      "  Training time: 4.78s\n",
      "  Top feature: US_Stock_VWO_adj_low_log_return (0.0116)\n",
      "\n",
      "Training model 4/10 for target_3...\n",
      "  Training samples: 1529\n",
      "  Features: 557\n",
      "  Training time: 4.76s\n",
      "  Top feature: US_Stock_BCS_adj_low_log_return (0.0118)\n",
      "\n",
      "Training model 5/10 for target_4...\n",
      "  Training samples: 1363\n",
      "  Features: 557\n",
      "  Training time: 4.66s\n",
      "  Top feature: US_Stock_XOM_adj_low_log_return (0.0130)\n",
      "\n",
      "Training model 6/10 for target_5...\n",
      "  Training samples: 1363\n",
      "  Features: 557\n",
      "  Training time: 4.55s\n",
      "  Top feature: US_Stock_ACWI_adj_close_log_return (0.0217)\n",
      "\n",
      "Training model 7/10 for target_6...\n",
      "  Training samples: 1529\n",
      "  Features: 557\n",
      "  Training time: 4.70s\n",
      "  Top feature: US_Stock_EFA_adj_high_log_return (0.0100)\n",
      "\n",
      "Training model 8/10 for target_7...\n",
      "  Training samples: 1456\n",
      "  Features: 557\n",
      "  Training time: 4.65s\n",
      "  Top feature: US_Stock_MPC_adj_high_log_return (0.0159)\n",
      "\n",
      "Training model 9/10 for target_8...\n",
      "  Training samples: 1328\n",
      "  Features: 557\n",
      "  Training time: 4.38s\n",
      "  Top feature: US_Stock_EWJ_adj_close_log_return (0.0310)\n",
      "\n",
      "Training model 10/10 for target_9...\n",
      "  Training samples: 1529\n",
      "  Features: 557\n",
      "  Training time: 4.84s\n",
      "  Top feature: US_Stock_KGC_adj_low_log_return (0.0105)\n"
     ]
    }
   ],
   "source": [
    "# Baseline model training (start with first 10 targets)\n",
    "print(\"Training baseline models...\")\n",
    "\n",
    "# Start with first 10 targets for quick testing\n",
    "test_targets = target_cols[:10]\n",
    "print(f\"Training models for targets: {test_targets}\")\n",
    "\n",
    "# Store results\n",
    "models = {}\n",
    "scores = {}\n",
    "training_times = {}\n",
    "feature_importance = {}\n",
    "\n",
    "# Note: Using single validation set approach (no cross-validation on training data)\n",
    "# This is more appropriate for time series data with proper train/validation splits\n",
    "\n",
    "for i, target in enumerate(test_targets):\n",
    "    print(f\"\\nTraining model {i+1}/{len(test_targets)} for {target}...\")\n",
    "    \n",
    "    # Get target data (remove missing values)\n",
    "    train_target_data = y_train[target].dropna()\n",
    "    train_feature_data = X_train.loc[train_target_data.index]\n",
    "    \n",
    "    print(f\"  Training samples: {len(train_target_data)}\")\n",
    "    print(f\"  Features: {train_feature_data.shape[1]}\")\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # Train model on all training data (no cross-validation)\n",
    "    model.fit(train_feature_data, train_target_data)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    models[target] = model\n",
    "    scores[target] = 0.0  # We'll calculate this on validation set\n",
    "    training_times[target] = training_time\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.feature_importances_\n",
    "    feature_importance[target] = pd.DataFrame({\n",
    "        'feature': train_feature_data.columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"  Top feature: {feature_importance[target].iloc[0]['feature']} ({feature_importance[target].iloc[0]['importance']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5249e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Performance Summary\n",
      "==================================================\n",
      "     target  training_time\n",
      "0  target_0       4.774156\n",
      "1  target_1       4.563421\n",
      "2  target_2       4.781083\n",
      "3  target_3       4.759797\n",
      "4  target_4       4.663609\n",
      "5  target_5       4.554591\n",
      "6  target_6       4.700098\n",
      "7  target_7       4.647001\n",
      "8  target_8       4.378689\n",
      "9  target_9       4.844430\n",
      "\n",
      "Average training time: 4.67s\n",
      "Total training time: 46.67s\n",
      "Models trained: 10\n",
      "\n",
      "Note: Model performance will be evaluated on validation set using competition metric\n"
     ]
    }
   ],
   "source": [
    "# Performance summary\n",
    "print(\"Baseline Model Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "performance_df = pd.DataFrame({\n",
    "    'target': list(models.keys()),\n",
    "    'training_time': list(training_times.values())\n",
    "})\n",
    "\n",
    "print(performance_df)\n",
    "print(f\"\\nAverage training time: {performance_df['training_time'].mean():.2f}s\")\n",
    "print(f\"Total training time: {performance_df['training_time'].sum():.2f}s\")\n",
    "print(f\"Models trained: {len(models)}\")\n",
    "\n",
    "# Note: Performance metrics will be calculated on validation set\n",
    "print(\"\\nNote: Model performance will be evaluated on validation set using competition metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53bc7df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Analysis\n",
      "==================================================\n",
      "Actual column names: ['level_0', 'level_1', 'feature', 'importance']\n",
      "\n",
      "Top 20 most important features overall:\n",
      "feature\n",
      "US_Stock_MPC_adj_low_log_return       0.007539\n",
      "US_Stock_NUGT_adj_close_log_return    0.005575\n",
      "US_Stock_VWO_adj_low_log_return       0.005164\n",
      "US_Stock_EWJ_adj_close_log_return     0.005075\n",
      "US_Stock_SPYV_adj_close_log_return    0.004545\n",
      "US_Stock_GDX_adj_close_log_return     0.004341\n",
      "US_Stock_MPC_adj_high_log_return      0.004272\n",
      "US_Stock_TRGP_adj_high_log_return     0.004153\n",
      "US_Stock_IGSB_adj_open_log_return     0.003968\n",
      "US_Stock_EWZ_adj_low_log_return       0.003938\n",
      "US_Stock_HES_adj_low_log_return       0.003574\n",
      "US_Stock_BNDX_adj_high_log_return     0.003428\n",
      "US_Stock_ACWI_adj_close_log_return    0.003415\n",
      "US_Stock_EWT_adj_close_log_return     0.003378\n",
      "US_Stock_JNK_adj_low_log_return       0.003353\n",
      "US_Stock_IEF_adj_close_log_return     0.003262\n",
      "US_Stock_EEM_adj_open_log_return      0.003158\n",
      "FX_AUDUSD_log_return                  0.003117\n",
      "US_Stock_TIP_adj_high_log_return      0.003068\n",
      "US_Stock_VCIT_adj_low_log_return      0.003039\n",
      "Name: importance, dtype: float32\n",
      "\n",
      "Top 5 features for each target:\n",
      "\n",
      "target_0:\n",
      "  US_Stock_MPC_adj_low_log_return: 0.0300\n",
      "  US_Stock_HES_adj_low_log_return: 0.0196\n",
      "  US_Stock_TIP_adj_high_log_return: 0.0163\n",
      "  US_Stock_EWZ_adj_low_log_return: 0.0155\n",
      "  US_Stock_EEM_adj_open_log_return: 0.0138\n",
      "\n",
      "target_1:\n",
      "  US_Stock_MPC_adj_low_log_return: 0.0254\n",
      "  US_Stock_SPYV_adj_close_log_return: 0.0123\n",
      "  US_Stock_GDXJ_adj_open_log_return: 0.0116\n",
      "  US_Stock_GDX_adj_high_log_return: 0.0105\n",
      "  US_Stock_GDX_adj_close_log_return: 0.0101\n",
      "\n",
      "target_2:\n",
      "  US_Stock_VWO_adj_low_log_return: 0.0116\n",
      "  US_Stock_GDXJ_adj_low_log_return: 0.0107\n",
      "  US_Stock_AGG_adj_low_log_return: 0.0089\n",
      "  US_Stock_BP_adj_low_log_return: 0.0088\n",
      "  US_Stock_EWZ_adj_close_log_return: 0.0083\n",
      "\n",
      "target_3:\n",
      "  US_Stock_BCS_adj_low_log_return: 0.0118\n",
      "  US_Stock_AMP_adj_close_log_return: 0.0088\n",
      "  US_Stock_GDX_adj_low_log_return: 0.0082\n",
      "  US_Stock_EEM_adj_low_log_return: 0.0076\n",
      "  US_Stock_CCJ_adj_close_log_return: 0.0074\n",
      "\n",
      "target_4:\n",
      "  US_Stock_XOM_adj_low_log_return: 0.0130\n",
      "  US_Stock_YINN_adj_low_log_return: 0.0129\n",
      "  US_Stock_GDX_adj_open_log_return: 0.0114\n",
      "  US_Stock_IEF_adj_close_log_return: 0.0101\n",
      "  US_Stock_VT_adj_high_log_return: 0.0096\n",
      "\n",
      "target_5:\n",
      "  US_Stock_ACWI_adj_close_log_return: 0.0217\n",
      "  US_Stock_BND_adj_close_log_return: 0.0148\n",
      "  US_Stock_YINN_adj_close_log_return: 0.0133\n",
      "  US_Stock_CVE_adj_close_log_return: 0.0108\n",
      "  US_Stock_IAU_adj_low_log_return: 0.0096\n",
      "\n",
      "target_6:\n",
      "  US_Stock_EFA_adj_high_log_return: 0.0100\n",
      "  US_Stock_LYB_adj_volume_log_return: 0.0096\n",
      "  US_Stock_BNDX_adj_high_log_return: 0.0093\n",
      "  US_Stock_VGLT_adj_close_log_return: 0.0092\n",
      "  JPX_Gold_Standard_Futures_Low_log_return: 0.0068\n",
      "\n",
      "target_7:\n",
      "  US_Stock_MPC_adj_high_log_return: 0.0159\n",
      "  US_Stock_BND_adj_low_log_return: 0.0151\n",
      "  US_Stock_VYM_adj_close_log_return: 0.0137\n",
      "  US_Stock_TRGP_adj_high_log_return: 0.0102\n",
      "  US_Stock_SPYV_adj_close_log_return: 0.0100\n",
      "\n",
      "target_8:\n",
      "  US_Stock_EWJ_adj_close_log_return: 0.0310\n",
      "  US_Stock_NUGT_adj_close_log_return: 0.0255\n",
      "  US_Stock_MPC_adj_high_log_return: 0.0170\n",
      "  US_Stock_VWO_adj_low_log_return: 0.0164\n",
      "  US_Stock_IEMG_adj_low_log_return: 0.0137\n",
      "\n",
      "target_9:\n",
      "  US_Stock_KGC_adj_low_log_return: 0.0105\n",
      "  US_Stock_EWT_adj_close_log_return: 0.0092\n",
      "  US_Stock_IGSB_adj_high_log_return: 0.0084\n",
      "  US_Stock_DVN_adj_high_log_return: 0.0082\n",
      "  US_Stock_HAL_adj_high_log_return: 0.0076\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine all feature importance\n",
    "all_importance = pd.concat(feature_importance.values(), keys=feature_importance.keys())\n",
    "all_importance = all_importance.reset_index()\n",
    "\n",
    "# Check the actual column names\n",
    "print(\"Actual column names:\", all_importance.columns.tolist())\n",
    "\n",
    "# Rename columns based on what we actually have\n",
    "if len(all_importance.columns) == 4:\n",
    "    # If we have 4 columns, it's likely: level_0, level_1, feature, importance\n",
    "    all_importance.columns = ['target', 'index', 'feature', 'importance']\n",
    "    # Drop the index column if it's not needed\n",
    "    all_importance = all_importance.drop('index', axis=1)\n",
    "elif len(all_importance.columns) == 3:\n",
    "    # If we have 3 columns, it's likely: level_0, feature, importance\n",
    "    all_importance.columns = ['target', 'feature', 'importance']\n",
    "else:\n",
    "    # Let's see what we actually have\n",
    "    print(\"Unexpected number of columns. Actual columns:\")\n",
    "    print(all_importance.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(all_importance.head())\n",
    "\n",
    "# Overall feature importance (average across targets)\n",
    "overall_importance = all_importance.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 most important features overall:\")\n",
    "print(overall_importance.head(20))\n",
    "\n",
    "# Feature importance by target\n",
    "print(\"\\nTop 5 features for each target:\")\n",
    "for target in test_targets:\n",
    "    print(f\"\\n{target}:\")\n",
    "    top_features = feature_importance[target].head(5)\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cea1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models and results...\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_0_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_1_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_2_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_3_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_4_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_5_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_6_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_7_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_8_model.joblib\n",
      "Saved model: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline/target_9_model.joblib\n",
      "Saved training summary: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results/baseline_training_summary.csv\n",
      "Saved feature importance: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results/feature_importance_baseline.csv\n",
      "Saved overall importance: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results/feature_importance_summary.csv\n",
      "\n",
      "Models and results saved successfully!\n",
      "Note: Performance evaluation will be done on validation set\n"
     ]
    }
   ],
   "source": [
    "# Save models and results\n",
    "print(\"Saving models and results...\")\n",
    "\n",
    "# Save trained models\n",
    "for target in models.keys():\n",
    "    model_path = MODELS_DIR / f'{target}_model.joblib'\n",
    "    joblib.dump(models[target], model_path)\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "\n",
    "# Save training summary (no performance metrics since we're not using CV)\n",
    "training_summary = pd.DataFrame({\n",
    "    'target': list(models.keys()),\n",
    "    'training_time': list(training_times.values())\n",
    "})\n",
    "training_summary_path = RESULTS_DIR / 'baseline_training_summary.csv'\n",
    "training_summary.to_csv(training_summary_path, index=False)\n",
    "print(f\"Saved training summary: {training_summary_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = RESULTS_DIR / 'feature_importance_baseline.csv'\n",
    "all_importance.to_csv(importance_path, index=False)\n",
    "print(f\"Saved feature importance: {importance_path}\")\n",
    "\n",
    "# Save overall importance summary\n",
    "overall_importance_path = RESULTS_DIR / 'feature_importance_summary.csv'\n",
    "overall_importance.to_frame().to_csv(overall_importance_path)\n",
    "print(f\"Saved overall importance: {overall_importance_path}\")\n",
    "\n",
    "print(f\"\\nModels and results saved successfully!\")\n",
    "print(f\"Note: Performance evaluation will be done on validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e10c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on validation set...\n",
      "Validation log returns shape: (90, 558)\n",
      "Validation feature matrix shape: (90, 557)\n",
      "Validation target matrix shape: (90, 424)\n",
      "Generating predictions for target_0...\n",
      "  RMSE: 0.0089, MAE: 0.0068, R²: -0.1727\n",
      "Generating predictions for target_1...\n",
      "  RMSE: 0.0132, MAE: 0.0107, R²: -0.1707\n",
      "Generating predictions for target_2...\n",
      "  RMSE: 0.0137, MAE: 0.0108, R²: -0.1189\n",
      "Generating predictions for target_3...\n",
      "  RMSE: 0.0153, MAE: 0.0120, R²: -0.0811\n",
      "Generating predictions for target_4...\n",
      "  RMSE: 0.0191, MAE: 0.0145, R²: -0.0704\n",
      "Generating predictions for target_5...\n",
      "  RMSE: 0.0245, MAE: 0.0191, R²: -0.1826\n",
      "Generating predictions for target_6...\n",
      "  RMSE: 0.0166, MAE: 0.0130, R²: -0.1059\n",
      "Generating predictions for target_7...\n",
      "  RMSE: 0.0169, MAE: 0.0127, R²: -0.0082\n",
      "Generating predictions for target_8...\n",
      "  RMSE: 0.0162, MAE: 0.0126, R²: -0.3046\n",
      "Generating predictions for target_9...\n",
      "  RMSE: 0.0154, MAE: 0.0122, R²: -0.0449\n",
      "\n",
      "Calculating competition score...\n",
      "Competition Score: -0.2180\n",
      "Score Interpretation: VERY POOR (negative or zero correlation)\n",
      "Saved validation predictions: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results/baseline/predictions.csv\n",
      "Saved validation metrics: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results/baseline/validation_metrics.csv\n",
      "Saved competition score: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results/baseline/competition_score.txt\n",
      "\n",
      "Validation predictions shape: (90, 11)\n",
      "Validation metrics calculated for 10 targets\n",
      "Competition Score: -0.2180\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on validation set with competition metric\n",
    "print(\"Generating predictions on validation set...\")\n",
    "\n",
    "# Use the already loaded validation data with log returns only\n",
    "val_feature_cols = [col for col in val_log_returns.columns if col != 'date_id']\n",
    "X_val = val_log_returns[val_feature_cols]\n",
    "y_val = val_labels[target_cols]\n",
    "\n",
    "print(f\"Validation log returns shape: {val_log_returns.shape}\")\n",
    "print(f\"Validation feature matrix shape: {X_val.shape}\")\n",
    "print(f\"Validation target matrix shape: {y_val.shape}\")\n",
    "\n",
    "# Generate predictions on validation set\n",
    "val_predictions = {}\n",
    "val_metrics = {}\n",
    "\n",
    "for target in models.keys():\n",
    "    print(f\"Generating predictions for {target}...\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    val_features_clean = X_val.fillna(0)\n",
    "    \n",
    "    # Ensure we have the same features as training\n",
    "    missing_features = set(feature_cols) - set(val_features_clean.columns)\n",
    "    if missing_features:\n",
    "        print(f\"  Warning: Missing {len(missing_features)} features, adding zeros\")\n",
    "        for feature in missing_features:\n",
    "            val_features_clean[feature] = 0\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    val_features_clean = val_features_clean[feature_cols]\n",
    "    \n",
    "    # Make predictions\n",
    "    val_predictions[target] = models[target].predict(val_features_clean)\n",
    "    \n",
    "    # Calculate traditional metrics\n",
    "    true_vals = y_val[target].dropna()\n",
    "    pred_vals = val_predictions[target][:len(true_vals)]\n",
    "    \n",
    "    if len(true_vals) > 0:\n",
    "        rmse = np.sqrt(mean_squared_error(true_vals, pred_vals))\n",
    "        mae = mean_absolute_error(true_vals, pred_vals)\n",
    "        r2 = r2_score(true_vals, pred_vals)\n",
    "        \n",
    "        val_metrics[target] = {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    else:\n",
    "        print(f\"  No valid target values for {target}\")\n",
    "\n",
    "# Create predictions DataFrame for competition metric\n",
    "val_predictions_df = pd.DataFrame(val_predictions)\n",
    "val_predictions_df['date_id'] = val_log_returns['date_id']\n",
    "val_predictions_df = val_predictions_df[['date_id'] + list(val_predictions.keys())]\n",
    "\n",
    "# Calculate competition score\n",
    "print(f\"\\nCalculating competition score...\")\n",
    "competition_score = calculate_competition_score(y_val, val_predictions_df[list(models.keys())])\n",
    "print(f\"Competition Score: {competition_score:.4f}\")\n",
    "print(f\"Score Interpretation: {interpret_competition_score(competition_score)}\")\n",
    "\n",
    "# Save validation predictions\n",
    "val_predictions_path = RESULTS_DIR / 'baseline' / 'predictions.csv'\n",
    "val_predictions_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "val_predictions_df.to_csv(val_predictions_path, index=False)\n",
    "print(f\"Saved validation predictions: {val_predictions_path}\")\n",
    "\n",
    "# Save validation metrics\n",
    "val_metrics_df = pd.DataFrame(val_metrics).T\n",
    "val_metrics_df.index.name = 'target'\n",
    "val_metrics_df.reset_index(inplace=True)\n",
    "val_metrics_path = RESULTS_DIR / 'baseline' / 'validation_metrics.csv'\n",
    "val_metrics_df.to_csv(val_metrics_path, index=False)\n",
    "print(f\"Saved validation metrics: {val_metrics_path}\")\n",
    "\n",
    "# Save competition score\n",
    "competition_score_path = RESULTS_DIR / 'baseline' / 'competition_score.txt'\n",
    "with open(competition_score_path, 'w') as f:\n",
    "    f.write(f\"Competition Score: {competition_score:.4f}\\n\")\n",
    "    f.write(f\"Interpretation: {interpret_competition_score(competition_score)}\")\n",
    "print(f\"Saved competition score: {competition_score_path}\")\n",
    "\n",
    "print(f\"\\nValidation predictions shape: {val_predictions_df.shape}\")\n",
    "print(f\"Validation metrics calculated for {len(val_metrics)} targets\")\n",
    "print(f\"Competition Score: {competition_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ff9e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Summary\n",
      "==================================================\n",
      "Models trained: 10\n",
      "Average training time: 4.67s\n",
      "Total training time: 46.67s\n",
      "\n",
      "Top 10 most important features:\n",
      " 1. US_Stock_MPC_adj_low_log_return: 0.0075\n",
      " 2. US_Stock_NUGT_adj_close_log_return: 0.0056\n",
      " 3. US_Stock_VWO_adj_low_log_return: 0.0052\n",
      " 4. US_Stock_EWJ_adj_close_log_return: 0.0051\n",
      " 5. US_Stock_SPYV_adj_close_log_return: 0.0045\n",
      " 6. US_Stock_GDX_adj_close_log_return: 0.0043\n",
      " 7. US_Stock_MPC_adj_high_log_return: 0.0043\n",
      " 8. US_Stock_TRGP_adj_high_log_return: 0.0042\n",
      " 9. US_Stock_IGSB_adj_open_log_return: 0.0040\n",
      "10. US_Stock_EWZ_adj_low_log_return: 0.0039\n",
      "\n",
      "Competition Score Results:\n",
      "Competition Score: -0.2180\n",
      "Interpretation: VERY POOR (negative or zero correlation)\n",
      "\n",
      "Next Steps:\n",
      "1. Analyze competition score and feature importance patterns\n",
      "2. Train models for all 425 targets\n",
      "3. Try using all engineered features (technical indicators, lagged features, etc.)\n",
      "4. Experiment with different algorithms (Random Forest, Neural Networks)\n",
      "5. Try factor models (PCA) for dimensionality reduction\n",
      "6. Ensemble different approaches\n",
      "7. Feature selection to reduce noise\n",
      "8. Hyperparameter tuning for better rank correlation\n",
      "\n",
      "Results saved to: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/results\n",
      "Models saved to: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/baseline\n",
      "Predictions saved to: /Users/fw1230/Documents/Projects/mitsui-commodity-prediction/models/predictions\n",
      "\n",
      "Key Insights:\n",
      "• Using log returns only as baseline (557 features)\n",
      "• Competition metric: Rank Correlation Sharpe Ratio\n",
      "• Time series split with gaps to prevent data leakage\n",
      "• Single validation approach (no cross-validation)\n"
     ]
    }
   ],
   "source": [
    "# Summary and next steps\n",
    "print(\"Baseline Model Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Models trained: {len(models)}\")\n",
    "print(f\"Average training time: {performance_df['training_time'].mean():.2f}s\")\n",
    "print(f\"Total training time: {performance_df['training_time'].sum():.2f}s\")\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "for i, (feature, importance) in enumerate(overall_importance.head(10).items()):\n",
    "    print(f\"{i+1:2d}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Load and display competition score if available\n",
    "competition_score_path = RESULTS_DIR / 'baseline' / 'competition_score.txt'\n",
    "if competition_score_path.exists():\n",
    "    with open(competition_score_path, 'r') as f:\n",
    "        score_content = f.read()\n",
    "        print(f\"\\nCompetition Score Results:\")\n",
    "        print(score_content)\n",
    "else:\n",
    "    print(f\"\\nNote: Run validation prediction cell to get competition score\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Analyze competition score and feature importance patterns\")\n",
    "print(\"2. Train models for all 425 targets\")\n",
    "print(\"3. Try using all engineered features (technical indicators, lagged features, etc.)\")\n",
    "print(\"4. Experiment with different algorithms (Random Forest, Neural Networks)\")\n",
    "print(\"5. Try factor models (PCA) for dimensionality reduction\")\n",
    "print(\"6. Ensemble different approaches\")\n",
    "print(\"7. Feature selection to reduce noise\")\n",
    "print(\"8. Hyperparameter tuning for better rank correlation\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "print(f\"Models saved to: {MODELS_DIR}\")\n",
    "print(f\"Predictions saved to: {PREDICTIONS_DIR}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"• Using log returns only as baseline (557 features)\")\n",
    "print(f\"• Competition metric: Rank Correlation Sharpe Ratio\")\n",
    "print(f\"• Time series split with gaps to prevent data leakage\")\n",
    "print(f\"• Single validation approach (no cross-validation)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commodity-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
